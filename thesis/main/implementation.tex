\chapter{Proposed Approach}

\section{Overview}

The application is the source identification, i.e. given a video we want to determine its origin. From which devices, services, platform program the video come from? Since the file container structure has a degree of arbitrariness, it has characteristics that can be used to identify the source.

For each of the origins, or classes, of interest, we ask a binary question, namely: does that video come from the source X? The answer must be a likelihood, measured in same way, that will express the degree to which the video belongs to that class; then, thresholding appropriately the likelihood, it will be possible to respond to the binary question.
For each question, two queries in the training dataset are made: we take a set of videos that we know belong to the class object of the question, i.e. for which the answer to the binary question will be true, and a complementary set of videos that we know do not belong to the class under examination, i.e. for which the answer is false.

Ground truth is then divided into these two sets. The, we merge all the file containers in a single XML file so that it contains all atoms and all possible values of the attributes of two sets. During the merge, the weight vector is initialized in the attribute string; each time a value is found, it set to zero its weight in the weights vector. When we find the same weight several times its weight increases, modulated by the total number of videos of the set. This computation serves to calculate the discriminative power of each attribute for the two classes.

At this point, given a video query as input, we want to determine its class, i.e. its source device. By looking at the values of its attributes for both classes, for each of these attributes we compute the ratio of the weights associated with that value in the two sets. The numerator represents the set that responds to true and the denominator the set that returns false. This ratio has a maximum of 1 and a minimum of zero excluded (see details). The ratios of all the attributes are then multiplied (which is why it cannot be zero). The more this product is greater than 1, the more the video is sure to belong to the set that responds true; the more it is less than 1, the more the video is sure to belong to the set that answers false.

\section{Video Format Tool}

\subsection{Functional Requirements}

In order to use the file containers as video analysis tools, we have implemented a program that allows extracting the file containers from an input video.
This program also presents various other features implemented both as low-level functions, to use a simple Java library, and as high-level functions, used as interfaces for the command-line program.

The implemented features are:
\begin{itemize}
\item[-] Parse: given input MP4-like format video file (MP4 and MOV), it extracts the file container using the library Mp4Parser and it saves it in an XML file.
\item[-] Batch parse: given as input a folder of video files, it parses all the videos in the folder and sub-folder, saving them by recreating the same folder structure.
\item[-] Draw: it is used to draw in a window a tree, given an XML file as input that represents a video file container.
\item[-] Merge: given two XML input files, it combines them into a single XML file. By taking one as the base for the merge, it adds to it the atoms that only the other XML file has; also, for atoms that are in common, it considers the attributes and, by looking at their values, it adds them to the base XML files, so that for each attributes we will have a vector of values.
\item[-] Update: it is an advanced method to use the merge. Instead of giving two XML files as input, it takes a folder that contains XML files and merges them into a single XML as explained above. It also considers sub-folders.
\item[-] Compare: given two XML files, it compares them and it returns a measure of how similar they are.
\end{itemize}

\subsection{Implementation} 
 
During the development of the application, we have made several implementation choices. In the following, it will be described the reason behind them, in addition to further details. The features that will be explained are only the main ones: parse, merge and appear:

\begin{itemize}

\item[-] Parse: the parse feature uses the \emph{Mp4Parser} library (cita), Java API to read, write and create MP4 files-like.
The high-level function is \emph{parse()}, which is used as an interface function for the command-line tool. It takes two input strings, the URL of the video file to parse and the path of the output directory where to save the created XML file. It works to make the setup for the operation, it calls the low-level \emph{parser()} function and it saves the extracted container in an XML file.

The \emph{parser()} function is what create the file container. It takes as input the ISO file of the video that we want to parse, extracted using the \emph{Mp4Parser} library. This function creates the root of the XML file (org.dom) and instantiates an object of class \emph{BoxParser}. This class is responsible for reading the ISO file recursively to extract all the atoms and their attributes. In fact, as explained above, the atoms have a nested structure. \emph{BoxParser} implements a recursive function called \emph{getBoxes()}. This work takes as input an \emph{AbstractContainerBox} and an \emph{Element} (jdom). The first parameter is passed as null in the first \emph{parser()} call and the second is the root element created in the \emph{parser()}.

The \emph{getBoxes()} function reads the ISO file (passed in the constructor of \emph{BoxParser}) and extracts atoms recursively. Initially, the \emph{AbstractContainerBox} is initialized to null, so the first time the atoms are obtained directly from the ISO files; these are the top-level atoms (ftyp, moov, mdat, etc.). In general, for each atom the children are extracted. It cycles on the children and, according to their 4-byte code represented by a 4-character string, it calls the right parser for that atom which can extract its attributes. Such "parser" is implemented as a \emph{Wrapper} interface that requires only a \emph{toString()} method. The wrappers implemented, in fact, call the get functions of the particular atom classes and build a string that forms the list of attributes and their value for a given atom. An attribute that has been artificially inserted in the implementation in all the atoms is the \emph{count} attribute with a fixed value to 0, which will subsequently be used only to verify if an atom is present or not. 
 
For unrecognized atoms, or for atoms for which it is not recognized the name of 4 characters, it is handled by a default wrapper; the task of such wrappers is limited only to insert the attribute count. You can always extend the number of supported atoms, merely implementing a new dedicated wrapper.

Once the atoms and the attributes are extracted, it is created XML element with the name equal to the 4-bye code and the attributes of the atom are set as a correspondent XML element attributes. At this point, if the atom has children, it is passed to the \emph{parser()} function, continuing the recursion. When the recursion reaches the end for a certain atom, the XML element is added to the root, which each time will be the XML element that had been passed in the recursion, up to the actual root. At this point, the ISO file parsing the video file is complete and the \emph{parser()} function has a root \emph{Element} that contains the file container represented as XML files. The \emph{parse()} function will save it to disk.

Some implementation details:
- To consider the position of the atoms in the container, a variable that assigns a number to each atom child is used, so as to assign an ordering to these children. This numbering is done so that it can be recognized the difference in position between atoms of two videos. The moov atom XML will no longer match the tag XML \emph{moov} but for example \emph{moov-2}. Two atoms \emph{moov} in a different position in the container are considered completely different and help significantly in the discrimination of video files using the file containers.
- The position of the attributes of atoms is not considered. Whether it exploited the \emph{toString()} function of \emph{Mp4Parser} library or the one implemented in the \emph{Wrappers}, the order of attributes is still arbitrarily decided by either one of these two proxies and has nothing to do with the choice of the manufacturer as is the case for the order of the atoms.

\item[-] Merge: this feature combines two XML files which represent the file containers. It will be used later during the training phase.

Given two XML file as inputs, the first is taken as the basis. The second is explored recursively and each atom is checked whether it is present or not in the base file. If it is present in both, it proceeds to compare the values of the attributes. If in the second file, it is found a different value for an attribute, it is added to the base file, and then creating a vector of values for that attribute. Along with a vector of values for each attribute, it is create a vector of weights, initialized to zero, the size same as the first vector. If it is not present, the atom of the second file is added to the base file along with its attributes.

The procedure then proceeds recursively, like in the parse feature. Since the merge produces an XML file as output, the merge can be done using an XML file of a video and the XML file produce as output by a previous merge. The only differences is that when will compare the values of the attributes, it will not have to check if the value is different, but rather if the value of the attribute for the second XML file is present in the vector of values for that attribute in the base file. 

\item[-] Compare: this feature serves to compare two XML files and give a measure of the difference of file containers they represent. The practical application is the Integrity Verification, i.e. determine whether the video is intact, in particular if its file container has not been altered after the acquisition device created it.

To compare two XML files, one is chosen as a reference and the other as a query. In theory, for the reference it is constructed a vector of size equal to the number of elements that can be different, namely all the attributes of all the atoms (for the atoms without attributes it will be considered only the count attribute). The elements of this vector are ideally initialized to 1. For the query is initialized a vector of the same size. Proceeding as before, the reference is explored recursively searching for difference with the query. If the atom is present, the attributes are compared, each different attribute corresponds to a zero in the query vector indicating a difference. If the atom is not present, then it will put as many zeros as the number of attributes for that particular atom of the reference.

In the end, we obtain two vectors of the same size, one of ones and the other of ones and zeros with a zero indicating a difference.
Since doing the Euclidean distance between two vectors of binary values, in which one is all initialized to one, equals to simply count the number of zeros that are present in the second vector and then computing the square root, instead of creating these vectors, we only incremented a variable for each difference found.
Besides the number of differences, it is also calculated the total number of attributes of the XML file reference. The final results will be a percentage of difference, compute by dividend the number of differences found with the total number of attributes.

The feature compare finally returns a JSON file with the following fields: reference, query, tot, diff, attributes.
\end{itemize}
 
In order to represent the XML files as objects, each XML file (DOM object) will be cast to a custom class called \emph{Tree}, so that they can be manipulated more easily accordingly to our needs. Each XML files tag is a \emph{Node} or a \emph{Leaf}, both classes that implements the \emph{Tree} interface. Each \emph{Tree} has variables for identification (the 4-byte code, converted as \emph{String}), a father (the root that is \emph{Null} object), a list of children trees and a list of fields, that correspond to the attributes of the atom.

\subsection{Command Line Tool}

`usage: vft [-b | -c | -d | -h | -m | -p | -u]    [-i <file|folder>] [-i2
       <file>]  [-o <folder>]   [-wa]
 -b,--batch                 batch parse a directory of video files; it
                            recreates the same folder structure
 -c,--compare               compare two xml files
 -d,--draw                  draw a tree from an xml file
 -h,--help                  print help message
 -i,--input <file|folder>   video file for --parse, xml file for --draw
                            and --compare, a folder for --batch and
                            --update-config
 -i2,--input2 <file>        second xml file for --merge and --compare
 -m,--merge                 merge two xml file into one
 -o,--output <folder>       output folder for the xml file,for --parse,
                            --merge and --update-config
 -p,--parse                 parse a video file container into a xml file
 -u,--update-config         merge all files in the dataset folder into a
                            config.xml file
 -wa,--with-attributes      whether to consider attributes in --merge and
                            --update-config or not. Default is false
`


\section{File Origin Analysis Tool}

\subsection{Functional Requirements}

L'implementazione della teoria è stata realizzata con un'applicazione java da utilizzare da terminale. Le funzionalità principale sono:
- Training: si occupa prendere i due set considerati e di fare il merge di tutti i file xml. Inoltre calcola il potere discriminante di ciascun attributo relativamente ad entrambi i set.
- Testing: è la fase che si occupa di rispondere alla domanda. Dato un video in ingresso, calcola la likelihood usando i config file costrutiti dalla fase di training. La likelihood viene modulata usando il logaritmo in base 10 e tale score viene sogliato sullo zero. >0 true, <0 false.

In foa sono state poi implementate una serie di operazione su database sqlite che serviranno successivamente per la web application che serve da interfaccia di foa e verranno quindi descritte in seguito.

\subsection{Implementation}

Sono state fatte alcune scelte implementative per migliorare la capacità del programa di identificare correttamente la classe:
- quando cerco un atomo per calcolare i rapporti, prendo l'atomo corrispondente nel config di training cercando il nome intero es. *<moov-2>*. Questo vale per tutti gli atomi tranne per *<trak>*. Il codice trak è usato sia per l'atomo delle traccia audio che quello della traccia video. Cercare *<trak-2>* potrebbe restituire si l'atomo con lo stesso nome ma che invece riguarda l'audio e non il video, dovuto al diverso posizionamente nel file container.
- alcuni attributi che sono noti essere unici per il video e non per la classe a cui appartiene non vengono considerati nella fase di testing. Tali attributi sono ad esempio la dimensione del file video, o la data di creazione e modifica. Potremmo considerarli e poi sogliare la likelihood in accordo dopo aver fatto una statistica di quanto sposta tale modifica per tutte le classi. (come in compare).
- alcuni atomi non vengono considerati, in particolare xyz e udta. L'atomo xyz è dove sono salvate le informazioni relative alla posizione del dispositivo nel momento dell'acquisizione. Sebbene molto utile per certi scopi, non è utile come discriminante per l'appartenza ad una classe di dispositivi. Infatti basta filmare due video dallo stesso dispositivo una volta col gps accesso e l'altra spento per ottenere due container diversi. Tale diversità è accentuata dal fatto che considerando la posizione degli atomi (numerazione dei figli), l'aggiunta di un tag fa incrementare il valore del numero a tutti gli atomi figli che vengono dopo. Questo significa che i container verrà visto come completamente diverso. Per porsi rimedia basta attivare una flag quando ho trovato ad un certo livello dell'albero creato dal file xml. Quando la flag è attiva, per quel livello del file xml, cerco i tag in base al codice vero senza considerare il numero della posizione. (da rivedere).
- dato un atomo ho tanti attributi; nella fase di testing per ogni attributo di un atomo calcolo il rapporto e poi ne faccio il prodotto. Ciò rappresenta la likelihood di quell'atomo. Se però i rapporti degli attributi di un atomo sono tutti esattamente uguali, praticamente sto moltiplicando più volte la stessa cosa; è come se stessi contanto più volte la stessa uguaglianza. Per ovviare è ciò la likelihood di ciascuno atomo è modulata in base ad un entropia data da una formula (da mettere). Quando l'entropia è massima, ovvero tutti i rapporti diversi fa il prodotto normale; quando è minima ovvero zero praticamente considera solo un rapport; nei casi medi fa una specie di media per quei gruppetti che hanno i rapporti uguali. Altrimenti alcuni atomi poco discriminanti ma con tanti attributi pesano tantissimo.
- rapporto: quando vado a calcolare il rapporto fra i pesi di un attributo di un atomo, vado a prendere i pesi nel set A e i pesi nel set B. Si possono verificare fra casi diversi:
 - numeratore e denominatore = 0: faccio rapporto normale.
 - numeratore = 0 e denominatore != 0: siccome non posso avere un rapporto pari a zero altrimenti col prodotto mi manda a zero tutta la likelihood, devo modificare il numeratore. Siccome tale caso in teoria è favorevole alla classe del denominatore (B), mi basta scegliere come numeratore un valore per il quale il rapporto è sempre minore di 1. Quindi scelgo 1 / il numero di video presenti nella classe B + 1. In tal modo è sempre minore di 1, con il rapporto che però varia in base al valore del denominatore.
 - numeratore != 0 e denominatore = 0: il denominatore non può essere zero. Come prima: mi basta che il rapporto sia sempre in favore di numeratore, quindi che sia sempre maggiore di 1. Sceglo il denominatore pari a 1 / il numero di video della classe A + 1. Anche in questo caso l'intensità di tale rapporto varia in base al valore del numeratore.
 - numeratore e denominatore != 0: questo caso si verifica quando il video di query presenta atomi o valori di attributi che non sono presenti nel config di training. Tale case viene considerato a favore della classe B. Infatti tale problema, seppure binario, non è simmetrico: nel dubbio dico che non è della classe A (meglio lasciare libero un colpevole che mettere condannare un innocente). Questo caso viene considerato come un 0/1, in cui il peso per la classe A è zero e quello per la classe B è pari a 1 massimo. Modifichiamo il numeratore con i principi del secondo caso, ma invece di considerare il numero di video della classe del denominatore, lo scegliamo di quello del denominatore. Infatti, seppure deciso in favore di B, tale caso è comunque dubbio quindi moduliamo il rapporto in base al numero di video di A.
 (Spiegare meglio discorso dei rapporti e dei numA ecc)

\subsection{Command Line Tool}

`usage: foa [-cA <xml file>] [-cB <xml file>] [-h | -init | -trn | -tst |
       -ute | -utr] [-i <xml/txt file or folder>]  [-lA <txt/json file>]
       [-lB <txt/json file>] [-o <folder>]     [-v]
 -cA,--configA <xml file>              xml config file for class A, only
                                       for --test
 -cB,--configB <xml file>              xml config file for class B, only
                                       for --test
 -h,--help                             print help message
 -i,--input <xml/txt file or folder>   xml file or txt file with list of
                                       xml paths for which compute the
                                       likelihood for class A and B, only
                                       for --test, dataset folder path for
                                       --update
 -init,--initialize                    initialize database
 -lA,--listA <txt/json file>           text/json file containing a list of
                                       xml file for class A, only for
                                       --train
 -lB,--listB <txt/json file>           text/json file containing a list of
                                       xml file for class B, only for
                                       --train
 -o,--output <folder>                  output folder for the training
                                       config files, only for --train
 -trn,--train                          train a binary classification
                                       problem
 -tst,--test                           predict the class of a xml file
 -ute,--update-testing                 update testing database
 -utr,--update-training                update trainingdatabase
 -v,--verbose                          whether or not display information,
                                       only for --test
`

\section{Web Application}

Per utilizzare più semplicemente gli strumenti di analisi forense implementati e spiegati precedentemente, è stato deciso di implementare una web application che funga da interfaccia e permettere ad un utente di customizzare le query e ottere un output dei risultati.
La web app è sviluppata come node.js app ed utilizza un server express. Segue quindi la struttura: html + css per aspetto, js client-side per interattività e ajax call, js server-side per rispondere alle chiamate ajax e eseguire le features e poi rispondere al client (node+epress).

Le features principali sono classify, per identificare la classe del dispositivo sorgente dato un video, compare, per integrity analysis, infine test per rendere più veloci i test con query.

Il dataset utilizzato è diviso in training e testing. Ciascun video possiede un file xml di informazioni utilizzato per costruire il groundtruth. Tali informazioni, insieme ai path del video, dell'info.xml e del file xml del container, sono salvati in un database. Tale database viene interrogato per ottenere i video per il training e per ottenere i video da testare.

\subsection{Features}

É possibile seleziona la features dal nav in cima alla pagina, con in aggiunta una pagina dove spiega come usare tali features.

- Classify:
questa features funziona in due modalità per il training, ovvero manuale e automatica. La modalità manuale si ha quando l'utente è a conoscenza del dispositivo sorgente del video query ma non ha il dispositivo ed intende verificare tale classe. La modalità automatica invece viene utilizzata quando l'utente è totalmente all'oscuro del dispositivo sorgente del video query. Le due modalità funzionano così:

 - manual:
  nel box class l'utente seleziona il brand, il model, e il sistema operativo. Questi elementi sono in sequenza; l'utente può scegliere solo il brand oppure il brand e il model oppure tutti e tre. In base a tale scelta cambierà come la classe avversaria (B) verrà selezionata, ed anche quella scelta (A).
  Per scegliere la classe A viene fatta un query al database sulla tabella dei video di training in base alla scelta della classe. Se scelgo solo il brand prendo tutti i video di quel brand; se scelgo brand e model prendo tutti i video di quel brand e specifico modello; infine se scelgo brand model e os prendo tutti i video di quel brand e modello con quel sistema operativo specificato.
  La classe B viene presa sulla base della scelta per la classe A. Se è stato scelto solo il brand prendo tutti i video i cui dispositivi non appartengono a quel brand; se è stato scelto solo brand e model, prendo tutti i video provenienti dai dispositivi di quel brand ma degli altri modelli disponibili per quel brand; infine se è stato scelto brand model e os, prendo tutti i video provenienti dai dispositivi di quel brand e modell ma degli altri sistemi operativi disponibili.
  Nel caso di brand model e os, se non sono disponibili altri sistemi operativi mi riporto al caso 2, ovvero brand e model. Nel caso di brand model, se non sono disponibili altri modelli per quel brand, mi riporto al caso 1, ovvero brand.

 - automatic:
  nel box class l'utente non seleziona nulla, lascia tutto any. A questo punto verranno testate tutte le classi disponibili nel database. Verrà viene seleziona ogni possibile classe A e di conseguenza ogni possibile classe B.

 Nel box upload, viene seleziona il video query che di cui si vuole predire la sorgente. È possibile caricare un video (mp4 o mov) o più video (max 5?), oppure direttamente i file xml dei rispettivi container. Nel caso in cui si caricano i file video, verrà fatto il parse del video prima di procedere.

 I file di training sono creati ed aggiornati periodicamente indipendentemente dalla query. Infatti tali file dipendono dai video presenti nel dataset; la scelta della classe da parte dell'utente serve solo a decide quali file di training utilizzare. Quindi è possibile ottimizzare creandoli prima e andando a selezionare quelli che mi servono di caso in caso.

 Una volta che ho caricato il video, scelto la classe A e B, il server utilizza i corretti file di training. Dopo c'è la fase di testing (utilizza foa), che, dati i video caricati, calcola la likelihood rispetto alla classe selezionata. Per il caso automatic verrà calcolata la likelihood per tutte le classi; tali likelihood vengono ordinate dal migliore al peggiore.

 Infine, finito tutto, nel box output viene mostrato il nome del video query e la classe testata insieme alla likelihood associata; per il caso automatic vengono mostrati, per ciascun video caricato, i migliori 5 risutalti (classe + likelihood).

- Test:
 serve per fare il classify più velocemente, senza dover caricare ogni volta i video. C'è solo il box class dove selezionare la classe da testare (manual o auto)e il box output. I video da testare vengono presi dal database dalla tabella dei video di testing, dove sono già parsati in file xml. Oltre all'output come prima, viene mostrata una statistica sui risultati della classificazione per tutti i video considerati (true positivi ecc, correct classification rate); infatti in questo caso è nota la classe vera dei video testati, classe (label) che viene mostrata accanto al nome del file video testato.

- Compare:
 il compare sfrutta la feature compare di vft. É possibile caricare due video, uno reference l'altro query dal box upload. Una volta finito, nel box output verranno mostrati i risultati, ovvero il numero totale di attributi di reference e le differenze rispetto a query; inoltre lo stesso calcolo è fatto invertendo ref e query, per dare una visione migliore delle differenze ed eventuali somiglianze/uguaglianze.

- vft-parse:
 se l'utente non è in possesso dei file container, può caricare un video; se però il file video è grande e non si può/vuole aspettare il caricamento sul server, dal box upload può essere scaricato un programmino jar chiamato vft-parse. Tale programma, con una semplice interfaccia in java swing, è una costola di vft completo e permetto di fare il parsing di un video selezionato o di un intera directory. In questo modo è possibile poi caricare i file xml invece dei file video, velocizzando notevolmente il processo per tutte le features.

- problema del parlante:
 il motivo per cui vengono scelte le classi avversarie nel modo spiegato precedentemente è che si va incontro al problema del parlante. Il problema del parlante consiste nel fatto che ad esempio nel riconoscimento del parlato, se uso un dataset molto vario in cui includo tutte le lingue e dialetti del modo per fare il training, poi non sono in grado di classificare accuratamente un parlato query; mentre se invece per fare il training seleziono dialetti simili allora sono in grado di classificare correttamente. Ciò sembra contro-intuitivo per la realtà, dove accade il contrare, ma in questi contesti usare cose simili significa dare molto più peso a piccole differenze che nel caso globale andrebbero perse. Questa è l'idea usata alla base della scelta delle classi avversarie, che cercano di essere diverse ma simili alla classe selezionata dall'utente.
